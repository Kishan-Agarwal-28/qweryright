# What is RAG? (Retrieval Augmented Generation)

LLMs (like GPT-4) are smart, but they have a problem: **They don't know your private data.**
If you ask ChatGPT, _"What was the total revenue for my company last Tuesday?"_ it will lie (hallucinate) or say it doesn't know.

**RAG** solves this using Qdrant.

---

## The RAG Workflow

1. **User asks a question:** _"What is our company's policy on remote work?"_
2. **Retrieval (Qdrant):** Your app takes that question, turns it into a vector, and searches Qdrant for the "3 most relevant paragraphs" in your 500-page HR manual.
3. **Augmentation:** Your app combines the question + the 3 paragraphs into one big message.
4. **Generation (LLM):** The LLM reads the paragraphs and answers the question based **ONLY** on that text.

---

## Real-World Example: Customer Support Bots

A company has 10,000 support tickets.
Instead of training a new AI model (which costs millions), they store all tickets in Qdrant.
When a customer asks a question, the bot "retrieves" similar old tickets and uses them to write the perfect answer.

---

## "Retrieval" is the Foundation

If Qdrant finds the **wrong** information, the LLM will give a **wrong** answer. This is why learning Qdrant (Distance metrics, indexing) is just as important as learning the AI itself!

---

## Quick Exercises:

1. **The Acronym**: What does "RAG" stand for?
2. **The Problem**: What is an LLM "Hallucination"? How does RAG help stop it?
3. **The Role**: In the RAG workflow, what is the specific job of Qdrant?
   hill.
