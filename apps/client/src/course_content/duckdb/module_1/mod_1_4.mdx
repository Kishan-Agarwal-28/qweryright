# Columnar Storage and Vectorized Execution

This is the "Secret Sauce." DuckDB isn't fast because it's new; it's fast because it uses two modern hardware optimizations.

---

## 1. Columnar Storage (The "What")

Traditional databases (Postgres, SQL Server) store data in **Rows**.

- To find the "Average Price" of 1 billion products, a row-based DB has to read the _name_, the _ID_, the _description_, and the _timestamp_ for every single product just to get the price.

DuckDB stores data in **Columns**.

- To find the "Average Price," DuckDB only reads the `price` column. Every other piece of data on the disk is ignored. This is a massive reduction in "Disk I/O."

---

## 2. Vectorized Execution (The "How")

Older databases process data **one row at a time**.

- Loop 1: Calculate $price \times tax$ for Row 1.
- Loop 2: Calculate $price \times tax$ for Row 2.
- Loop 3: ... 1 billion times.

DuckDB uses **Vectorized Execution**.

- It processes "Vectors" (chunks) of data at once (e.g., 2,048 rows at a time).
- It tells the CPU: "Multiply all 2,048 numbers in Vector A by all 2,048 numbers in Vector B."
- Modern CPUs (Intel, AMD, Apple Silicon) have special instructions for this, allowing them to do the math in a single heart-beat.

---

## 3. Compression

Because columns contain data of the same type (e.g., a column of all "Years" or all "State Initials"), DuckDB can compress them incredibly well.

- A table that takes 10GB in Postgres might only take 2GB in DuckDB.

---

## Real-World Example: Log Filtering

You have 100 million logs, each with a `level` column ("INFO", "ERROR", "DEBUG").

- **Row DB:** Scans all 100 million full log lines.
- **DuckDB:** Only scans the `level` column. Since "ERROR" is short and repetitive, DuckDB uses "Dictionary Compression." It only reads a tiny fraction of the data to find all the errors.

---

## Hypothetical Scenario: The "Wide Table"

A marketing table has 500 columns of data for every customer.

- You run `SELECT avg(spending) FROM customers;`.
- **Row DB:** Performance is terrible because it has to skip over 499 useless columns for every row.
- **DuckDB:** Performance is perfect because "Width" doesn't matter. It only touches the 1 column you asked for.

## Exercises

1. **Definition:** What is the difference between row-based and column-based storage?
2. **CPU Logic:** Why is processing 2,048 rows at once (vectorization) faster than processing them 1 by 1?
3. **Storage:** Why does columnar data compress better than row-based data?

## Summary and Next Steps

Columnar storage saves your Hard Drive; Vectorization saves your CPU.

Now that we understand the engine, let's look at how it actually writes data to your computer in **The .db File Persistence**.
